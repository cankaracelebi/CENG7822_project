\documentclass[11pt, a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{xcolor}

\geometry{margin=1in}

% Title
\title{\textbf{Applied RL: 2D Shooter Game\\Final Report}}
\author{Can Kara√ßelebi}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}

This report presents my work on training reinforcement learning agents for a custom 2D top-down shooter game environment. The core objective is to develop autonomous agents capable of navigating a dynamic game world, collecting prizes for positive rewards, avoiding or eliminating enemies, and surviving for extended periods. We investigate multiple model-free reinforcement learning algorithms including Deep Q-Network (DQN), Proximal Policy Optimization (PPO), and Soft Actor-Critic (SAC), evaluating their performance across different reward shaping strategies and training durations. Additionally, we have implemented a world model approach using Variational Autoencoders (VAE) and MDN-RNN for latent space representation learning and imagination-based planning. My experimental design encompasses 27 distinct configurations, systematically varying the algorithm choice, reward shaping parameters, and total training timesteps. This comprehensive training regime allows us to draw meaningful conclusions about the relative effectiveness of different approaches and the importance of reward engineering in shaping agent behavior. The results indicate that reward shaping plays a critical role in determining final performance with aggressive reward configurations that emphasize positive feedback consistently outperforming its alternatives.

\section{Literature Survey}

We survey prior work in deep reinforcement learning (RL) across several themes relevant to our project: RL in shooting game environments (2D and 3D), reward shaping for improved learning, safe exploration perspectives and generalization to new scenarios. Throughout, we position our custom 2D top-down shooter as a course-scale, analyzable environment that inherits the core challenges of aim, move and survive while enabling systematic ablations.

\subsection{Deep RL in Arcade Games}

Early breakthroughs in deep RL demonstrated that agents can learn complex 2D arcade games directly from raw pixels. The DQN line of work showed that a single deep network can learn effective control policies for Atari shooters (e.g., Space Invaders) using only screen input and game score, without game-specific feature engineering \cite{mnih2015dqn}. This result established a compelling baseline for end-to-end visual RL in action-heavy settings. Our environment builds on this foundation with a smaller but more dynamic 2D arena where enemy motion and prize spawn patterns vary episode-to-episode, shifting difficulty from raw perception toward continuous tactical trade-offs.

\subsection{3D Shooter Environments}

Moving from 2D arcade to 3D shooter research introduces partial observability, large action spaces, and harder exploration. ViZDoom formalized Doom-based FPS scenarios as a visual RL platform and competition benchmark \cite{kempka2016vizdoom}. Subsequent agents in Doom-style tasks highlighted the need to jointly learn navigation and combat behaviors under limited view and sparse rewards \cite{lample2017playing}. Unity and ML-Agents further popularized modular, reproducible game-like RL experimentation across 2D/3D settings \cite{juliani2018unity}. Our top-down 2D environment preserves the core shooter loop (seek threats, aim, shoot, dodge) while removing first-person observability constraints, making it suitable for faster iteration and cleaner ablation of reward and randomization choices.

\subsection{Reward Shaping Theory}

Reward design is central in shooter tasks. Theoretical work on reward shaping formalized when additional shaping terms can accelerate learning without changing the optimal policy under specific conditions \cite{ng1999policy}. In practice, shooter-like tasks often benefit from denser intermediate feedback (for hits, pickups, or survival) rather than purely terminal win/loss signals. We construct a course-appropriate, interpretable composite reward that encodes task decomposition: prize acquisition, threat reduction, and survival. We treat reward engineering as an experimental object and conduct ablations by scaling or removing components to observe behavioral shifts.

\subsection{Safe Reinforcement Learning}

A complementary lens is safety. Safe RL surveys emphasize maximizing return while also accounting for safety constraints or risk-aware exploration during learning and deployment \cite{garcia2015safe}. Even in simulated games, this framing is useful because reckless exploration can lead to training instability and brittle policies. We form an implicit safety bias through strong negative events (damage, death) and additional reporting metrics (survival time, damage per episode). While we do not claim formal guarantees, we use the safe RL lens to structure evaluation beyond raw reward.

\subsection{Generalization in RL}

Generalization is another key concern. RL agents can overfit to fixed training conditions and fail under modest changes \cite{cobbe2019generalization}. Domain randomization provides a pragmatic route to robustness by training on distributions of environment parameters \cite{tobin2017domain}. We implement lightweight domain randomization within a single arena family by varying enemy speed, spawn locations, and prize frequency.

\subsection{World Models and Model-Based RL}

Model-based reinforcement learning offers a compelling alternative to model-free methods by learning a predictive model of the environment. Rather than learning a policy purely from trial-and-error, model-based agents can ``imagine'' potential futures and plan actions accordingly. This section surveys the evolution from early world models to modern approaches like DreamerV2.

\subsubsection{The Original World Models Framework}

Ha and Schmidhuber \cite{worldmodels} introduced a three-component architecture that became foundational for subsequent work:

\begin{enumerate}
    \item \textbf{Vision Model (V)}: A Variational Autoencoder compresses high-dimensional visual observations into compact latent codes $z_t$. The VAE is trained to minimize reconstruction loss plus KL divergence, ensuring the latent space is both informative and well-structured.
    
    \item \textbf{Memory Model (M)}: An MDN-RNN (Mixture Density Network combined with LSTM) learns temporal dynamics by predicting $P(z_{t+1} | z_t, a_t, h_t)$ as a mixture of Gaussians. The LSTM hidden state $h_t$ captures historical context.
    
    \item \textbf{Controller (C)}: A small linear controller maps the concatenation of $z_t$ and $h_t$ to actions. Crucially, the controller can be trained entirely within the ``dream'' of the world model using evolutionary strategies.
\end{enumerate}

This staged training approach (V $\rightarrow$ M $\rightarrow$ C) demonstrated that agents could learn effective policies for complex tasks like car racing primarily through imagination, with minimal real-world interaction during controller training.



\subsubsection{Dreamer Series: Evolution of World Models}

The Dreamer algorithm family \cite{dreamerv1, dreamerv2, dreamerv3} advanced world models by training actor-critic agents purely in imagination. DreamerV1 utilized continuous latent states, while DreamerV2 introduced discrete categorical latents to better model non-continuous game events, using straight-through gradients for differentiability. DreamerV3 further achieved domain-agnostic performance using symlog predictions to handle diverse reward scales and varying signal magnitudes without tuning. Our implementation draws primarily from the World Models architecture but incorporates insights from Dreamer's latent space design.

\subsubsection{COOM: A Benchmark for Continual Reinforcement Learning}

COOM (Continual DOOM) \cite{coom} represents an important recent benchmark for evaluating reinforcement learning agents in complex 3D environments. Presented at NeurIPS 2023 by Tomilin et al., COOM specifically targets \textit{continual reinforcement learning}--the ability of agents to learn new tasks sequentially while retaining knowledge of previously learned ones.\\

\noindent
\textbf{Benchmark Design :} Unlike traditional RL benchmarks that evaluate single-task performance, COOM assesses three crucial aspects of continual learning:
\begin{enumerate}
    \item \textbf{Catastrophic Forgetting}: Does performance on earlier tasks degrade after learning new ones?
    \item \textbf{Knowledge Transfer}: Can learned skills accelerate acquisition of related tasks?
    \item \textbf{Sample Efficiency}: How quickly can agents adapt to new scenarios?
\end{enumerate}

\textbf{Environment Scenarios:} Built on the ViZDoom platform, COOM provides 8 distinct scenarios with diverse objectives:
\begin{itemize}
    \item \textbf{Run and Gun}: Navigate and eliminate enemies using ranged weapons
    \item \textbf{Chainsaw}: Seek and melee enemies in close combat
    \item \textbf{Hide and Seek}: Evade pursuing enemies as long as possible
    \item \textbf{Floor is Lava}: Platform navigation with dynamically appearing safe zones
    \item \textbf{Pitfall}: Traverse tunnels while avoiding bottomless pits
    \item \textbf{Arms Dealer}: Collect and deliver weapons to marked locations
    \item \textbf{Raise the Roof}: Locate switches to prevent ceiling crush hazards
    \item \textbf{Health Gathering}: Survival through health kit collection
\end{itemize}

\noindent
Each scenario offers default and hard difficulty variants, modifying agent speed, enemy count, or environmental complexity.

\textbf{Task Sequence Types:} COOM formulates continual learning through two distinct sequence types:
\begin{itemize}
    \item \textbf{Cross-Domain (CD)}: Same objective across visual variants--the agent trains on modified versions of Run and Gun with different textures, enemy types, and obstacles
    \item \textbf{Cross-Objective (CO)}: Different scenarios with novel objectives--the goal changes from elimination (Run and Gun) to evasion (Hide and Seek) to navigation (Floor is Lava)
\end{itemize}

Sequences come in 4-task, 8-task, and 16-task variants for varying evaluation complexity.

\textbf{Evaluation Metrics:} COOM defines rigorous quantitative metrics for assessing continual learning:

\begin{itemize}
    \item \textbf{Average Performance (AP)}: Mean success rate across all tasks at sequence completion:
    \begin{equation}
        AP = \frac{1}{N} \sum_{i=1}^{N} R_i^{(N)}
    \end{equation}
    where $R_i^{(N)}$ is performance on task $i$ after training on all $N$ tasks.
    
    \item \textbf{Forgetting (FGT)}: Maximum performance drop on a task after learning subsequent tasks:
    \begin{equation}
        FGT_i = \max_{j \in \{i+1, \ldots, N\}} \left( R_i^{(i)} - R_i^{(j)} \right)
    \end{equation}
    High forgetting indicates catastrophic interference between tasks.
    
    \item \textbf{Forward Transfer (FWT)}: Training efficiency on task $i$ relative to learning from scratch:
    \begin{equation}
        FWT_i = \frac{\text{AUC}_{\text{continual}}}{\text{AUC}_{\text{scratch}}} - 1
    \end{equation}
    Positive FWT indicates beneficial transfer; negative indicates \textit{negative transfer}.
\end{itemize}

\textbf{Baseline Results:} COOM evaluates several continual learning methods:
\begin{table}[H]
\centering
\caption{COOM Baseline Performance on Cross-Objective 8-task Sequence}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{AP $\uparrow$} & \textbf{FGT $\downarrow$} & \textbf{FWT $\uparrow$} \\
\midrule
Fine-tuning & 0.31 & 0.58 & -0.12 \\
EWC & 0.35 & 0.51 & -0.08 \\
PackNet & 0.42 & 0.24 & -0.15 \\
ClonEx-SAC & \textbf{0.61} & \textbf{0.18} & \textbf{0.05} \\
\bottomrule
\end{tabular}
\label{tab:coom-baselines}
\end{table}

Key findings from baseline evaluation:
\begin{itemize}
    \item Even the best method (ClonEx-SAC) shows 18\% forgetting
    \item Forward transfer is often \textbf{negative}--prior tasks can hurt new learning
    \item Cross-objective sequences are significantly harder than cross-domain variants
    \item Visual complexity alone does not fully explain performance degradation
\end{itemize}

\noindent
\textbf{Relevance to Our Work:} COOM's design principles directly inform our shooter environment and world model implementation:
\begin{enumerate}
    \item \textbf{Multi-objective Nature}: Our environment requires agents to simultaneously optimize survival, collection, and combat--similar to how COOM scenarios test diverse skills.
    
    \item \textbf{World Models for Continual Learning}: The benchmark demonstrates that continual learning in shooter-like environments remains an open challenge. World model approaches might enable more robust skill retention through learned dynamics that transfer across tasks.
    
    \item \textbf{Evaluation Framework}: COOM's metrics (AP, FGT, FWT) provide a principled way to evaluate how well our agents might generalize to new scenarios or retain learned behaviors after environment modifications.
    
    \item \textbf{Visual RL Challenges}: Both COOM and our visual world model approach face the challenge of extracting task-relevant information from high-dimensional pixel observations.
\end{enumerate}

\subsubsection{Training World Models: Key Considerations}

Training world models effectively requires attention to several factors:

\begin{enumerate}
    \item \textbf{Data Collection Strategy}: Random policies provide diverse coverage but may miss important states. Using trained policies risks distribution mismatch. Iterative approaches alternate between data collection and model training.
    
    \item \textbf{Model Compounding Error}: Multi-step imagination accumulates prediction errors. Shorter imagination horizons are more accurate but limit long-term planning. DreamerV2 uses 15-step imagination horizons.
    
    \item \textbf{Reconstruction vs. Prediction}: VAEs optimize reconstruction, but planning requires accurate dynamics. The world model loss balances these objectives through the KL term.
    
    \item \textbf{Latent Space Structure}: Well-structured latent spaces (smooth, disentangled) enable better planning. Categorical representations in DreamerV2 provide discrete structure that aids exploration.
\end{enumerate}

\section{Environment Description}

The shooter environment is a gymnasium-compliant 2D simulation where an agent must navigate, collect prizes, and avoid or destroy enemy entities. The environment presents several challenging aspects that make it suitable for RL research: continuous state dynamics, multi-objective optimization (survival vs. prize collection vs. enemy elimination), and the need for both reactive and strategic decision-making.

\subsection{State Space Representation}

The observation space is a normalized vector containing complete information about the game state from the agent's perspective. The agent receives its own position and velocity encoded as four continuous values normalized to the range $[0, 1]$. Health and shooting cooldown provide two additional state dimensions that inform resource management decisions. For each of the $k$ nearest enemies (typically $k=3$), the agent observes relative position and velocity components, contributing $4k$ dimensions. Similarly, the $m$ nearest prizes are represented by their relative positions, adding $2m$ dimensions. This vector-based representation provides sufficient information for decision-making while maintaining computational efficiency.

\subsection{Action Space Design}

The action space follows a multi-discrete structure with three independent components that the agent selects simultaneously at each timestep. The movement component offers five options: staying stationary or moving in one of the four cardinal directions. The shooting component is binary, determining whether the agent fires a projectile. The direction component specifies the aim direction from eight equally-spaced angles around the agent. This multi-discrete formulation creates 80 unique action combinations ($5 \times 2 \times 8 = 80$). Different RL algorithms handle this complexity differently: PPO can work directly with multi-discrete actions, DQN requires flattening to a single discrete space of 80 actions, and SAC requires a continuous action wrapper that maps box-constrained continuous values back to discrete choices.

\subsection{Reward Shaping Configurations}

Reward shaping is a powerful technique for guiding agent learning, and our experiments systematically explore three distinct philosophies encoded as reward configurations. Each configuration represents a different balance between encouraging positive achievements and penalizing negative outcomes.

\begin{table}[H]
\centering
\caption{Reward Configuration Parameters. Positive values encourage behavior; negative values discourage. Each component provides a signal at the corresponding event during gameplay.}
\label{tab:reward-config}
\begin{tabular}{lccc}
\toprule
\textbf{Reward Component} & \textbf{Baseline} & \textbf{Survival} & \textbf{Aggressive} \\
\midrule
Prize Collection ($R_{prize}$) & +1.0 & +0.5 & +2.0 \\
Enemy Hit ($R_{hit}$) & +0.3 & +0.1 & +0.5 \\
Enemy Kill ($R_{kill}$) & +1.0 & +0.5 & +2.0 \\
Damage Taken ($R_{damage}$) & $-$1.0 & $-$3.0 & $-$0.5 \\
Shot Fired ($R_{shot}$) & $-$0.02 & $-$0.05 & $-$0.01 \\
Time Step ($R_{time}$) & $-$0.001 & $-$0.0005 & $-$0.002 \\
Death Penalty ($R_{death}$) & $-$5.0 & $-$10.0 & $-$3.0 \\
\bottomrule
\end{tabular}
\end{table}

\noindent
The \textbf{baseline} configuration represents a balanced approach with moderate rewards and penalties. The survival configuration heavily penalizes damage and death while offering smaller positive rewards, intended to train risk-averse agents that prioritize staying alive. The aggressive configuration maximizes positive rewards while minimizing penalties, encouraging agents to actively engage with the environment even at the cost of taking damage.

\section{Experimental Setup}

\subsection{Algorithm Selection and Implementation}

We evaluate three state-of-the-art reinforcement learning algorithms that represent different paradigms in the field. Our implementations leverage the Stable-Baselines3 library with custom wrappers for action space compatibility. Deep Q-Network (DQN) is an off-policy value-based method that maintains a replay buffer of past experiences and trains a neural network to estimate action values. DQN uses epsilon-greedy exploration, starting with high randomness and gradually transitioning to predominantly greedy action selection. For our multi-discrete action space, we flatten all 80 combinations into a single discrete space. DQN trains efficiently at approximately 800 frames per second on CPU. Proximal Policy Optimization (PPO) is an on-policy policy gradient method that directly optimizes the policy while preventing destructively large updates through a clipped surrogate objective. PPO naturally supports multi-discrete action spaces and demonstrates stable learning dynamics. We use 4 parallel environments for data collection, achieving approximately 1500 frames per second aggregate throughput.

\textbf{Soft Actor-Critic (SAC)} is an off-policy actor-critic method that maximizes both reward and entropy, encouraging exploration through its maximum entropy framework. SAC requires continuous action spaces, so we wrap the environment to accept box-constrained continuous actions that are then discretized. Due to its sample-efficient but computationally intensive nature, SAC trains at approximately 65 frames per second, significantly slower than the alternatives.

\subsection{Training Duration Configurations}

To understand how learning progresses over time and whether additional training yields proportional improvements, we evaluate three training duration settings. The \textbf{short} configuration runs for 50,000 timesteps, providing a quick assessment of initial learning dynamics. The \textbf{medium} configuration extends to 500,000 timesteps, allowing substantial policy refinement. The \textbf{long} configuration runs for 1,600,000 timesteps, pushing toward asymptotic performance.

\section{Experimental Results}

\subsection{Learning Curve Analysis}

The learning curves reveal distinct patterns for each algorithm and reward configuration combination. All figures show episode reward (y-axis) versus training timesteps (x-axis), with shaded regions indicating $\pm 1$ standard deviation across multiple evaluation runs. Figure~\ref{fig:dqn-curves} shows DQN's progression across the three reward configurations. Notably, DQN with survival rewards exhibits a reward decrease after initial improvement in the medium-duration training. In my opinion, this phenomenon occurs because the survival configuration's heavy death penalty ($-10.0$) creates a sparse reward landscape where the agent initially learns to avoid immediate death but subsequently struggles to accumulate positive rewards, leading to policy degradation as the agent becomes overly conservative.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{report_figures/dqn_learning_curves.png}
\caption{DQN Learning Curves across reward configurations. X-axis: training timesteps (0--1.6M). Y-axis: episode reward. Each panel shows a different reward configuration (baseline, survival, aggressive). Curves represent different training durations. The survival configuration shows reward decrease due to sparse positive signals and heavy penalties.}
\label{fig:dqn-curves}
\end{figure}

\noindent
PPO demonstrates the most stable learning behavior among the three algorithms, as shown in Figure~\ref{fig:ppo-curves}. The policy gradient updates produce smooth reward improvements without the oscillations sometimes observed in value-based methods. PPO with aggressive rewards and long training achieves our best observed performance of +38.74 mean reward.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{report_figures/ppo_learning_curves.png}
\caption{PPO Learning Curves. X-axis: training timesteps (0--1.6M). Y-axis: episode reward. PPO exhibits smooth, monotonic improvement across all configurations due to its clipped objective preventing catastrophic policy updates. The aggressive configuration with 1.6M timesteps achieves the highest performance.}
\label{fig:ppo-curves}
\end{figure}

\noindent
SAC's learning curves in Figure~\ref{fig:sac-curves} show more modest improvements compared to DQN and PPO. The entropy regularization may interfere with exploitation in our relatively simple environment, and the continuous-to-discrete action wrapper introduces additional complexity.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{report_figures/sac_learning_curves.png}
\caption{SAC Learning Curves. X-axis: training timesteps (0--1.6M). Y-axis: episode reward. SAC shows slower improvement rates compared to other algorithms due to action space mismatch and entropy regularization. The aggressive configuration still outperforms survival, demonstrating reward shaping's universal importance.}
\label{fig:sac-curves}
\end{figure}

\subsection{Algorithm Comparison}

Figure~\ref{fig:algo-compare} directly compares the three algorithms on each reward configuration using the medium (500k) timestep setting. This controlled comparison reveals that DQN and PPO perform comparably on baseline and aggressive configurations, while both significantly outperform SAC.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{report_figures/algorithm_comparison.png}
\caption{Algorithm Comparison at 500k timesteps. X-axis: reward configuration (baseline, survival, aggressive). Y-axis: mean episode reward. Bars represent DQN (blue), PPO (green), and SAC (red). Error bars show $\pm 1$ standard deviation. DQN and PPO achieve similar performance, with both significantly outperforming SAC across all configurations.}
\label{fig:algo-compare}
\end{figure}

\subsection{Summary Statistics and Discussion}

Table~\ref{tab:results} presents the complete numerical results for all experimental configurations. The mean reward is computed over the final 10\% of training episodes to represent converged performance. Standard deviations are reported to quantify result variability.

\begin{table}[H]
\centering
\caption{Final Performance Across All Configurations (Mean $\pm$ Std over final 10\% of episodes). High standard deviations reflect the stochastic nature of individual episodes--enemy spawns, prize locations, and combat outcomes vary significantly between runs.}
\label{tab:results}
\begin{tabular}{llcrc}
\toprule
\textbf{Algorithm} & \textbf{Reward} & \textbf{Steps} & \textbf{Episodes} & \textbf{Mean Reward} \\
\midrule
DQN & Aggressive & Long & 3,328 & $+33.95 \pm 28.12$ \\
DQN & Aggressive & Medium & 1,390 & $+13.32 \pm 13.31$ \\
DQN & Baseline & Long & 2,997 & $+19.42 \pm 17.84$ \\
DQN & Baseline & Medium & 1,537 & $+0.70 \pm 5.75$ \\
DQN & Survival & Long & 2,714 & $-8.77 \pm 6.09$ \\
DQN & Survival & Medium & 1,295 & $-11.68 \pm 3.52$ \\
\midrule
PPO & Aggressive & Long & 3,065 & $\mathbf{+38.74 \pm 30.61}$ \\
PPO & Aggressive & Medium & 1,535 & $+14.37 \pm 15.12$ \\
PPO & Baseline & Long & 2,952 & $+21.96 \pm 16.92$ \\
PPO & Baseline & Medium & 1,468 & $+4.08 \pm 8.19$ \\
PPO & Survival & Long & 3,225 & $-8.80 \pm 6.61$ \\
PPO & Survival & Medium & 1,506 & $-13.39 \pm 1.29$ \\
\midrule
SAC & Aggressive & Long & 3,978 & $+1.49 \pm 4.96$ \\
SAC & Aggressive & Medium & 2,161 & $+2.23 \pm 5.71$ \\
SAC & Baseline & Long & 7,714 & $-5.02 \pm 1.75$ \\
SAC & Baseline & Medium & 2,246 & $-3.90 \pm 2.83$ \\
SAC & Survival & Long & 6,906 & $-13.73 \pm 0.81$ \\
SAC & Survival & Medium & 2,229 & $-13.54 \pm 0.94$ \\
\bottomrule
\end{tabular}
\end{table}

\noindent
\textbf{On High Standard Deviations:} The relatively high standard deviations observed (e.g., $\pm 30.61$ for PPO aggressive/long) deserve explanation. These reflect the inherent stochasticity of the game environment rather than training instability. Each episode involves random enemy spawns, prize locations, and combat outcomes. An agent might collect 5 prizes in a ``lucky'' episode or encounter difficult enemy configurations leading to early death. The aggressive reward configuration amplifies this variability because both positive rewards (+2.0 per prize) and negative events are magnified. Importantly, the \textit{mean} rewards are statistically significant--even with high variance, PPO aggressive/long ($+38.74$) clearly outperforms PPO survival/long ($-8.80$) beyond the overlap of their standard deviations.

\section{Ablation Study: Effect of Reward Shaping (Engineering) }

Our ablation study reveals that reward shaping is the most influential factor in determining final agent performance. Figure~\ref{fig:ablation} presents a grouped bar chart comparing all algorithms across the three reward configurations at 500k timesteps.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{report_figures/reward_ablation.png}
\caption{Reward Shaping Ablation Study. X-axis: algorithm (DQN, PPO, SAC). Y-axis: mean episode reward at 500k timesteps. Bars grouped by reward configuration: baseline (blue), survival (orange), aggressive (green). The aggressive configuration consistently achieves the highest performance across all algorithms, demonstrating that reward design dominates algorithm choice.}
\label{fig:ablation}
\end{figure}

\subsection{Why Aggressive Rewards Work Best}

The aggressive reward configuration's success can be attributed to several interconnected factors. First, the higher positive rewards for prize collection (+2.0 vs +1.0 baseline) and enemy kills (+2.0 vs +1.0) create stronger gradient signals that more effectively guide policy updates. The agent receives clear reinforcement for desirable behaviors, accelerating learning convergence. Second, the reduced death penalty ($-$3.0 vs $-$5.0 baseline) allows agents to learn from risky exploratory behaviors without catastrophically negative feedback. This encourages the agent to attempt challenging maneuvers that might occasionally result in death but ultimately lead to discovering more effective strategies. Third, the higher time penalty ($-$0.002 vs $-$0.001) discourages passive play, pushing agents toward active engagement with the environment rather than simply waiting and avoiding risk.

\subsection{Why Survival Rewards Struggle}

The survival configuration's poor performance across all algorithms highlights the limitations of penalty-dominated reward functions. The heavy death penalty ($-$10.0) creates a sparse reward landscape where avoiding death becomes the primary objective, but the path to survival offers little positive reinforcement. The reduced positive rewards (+0.5 for prizes and kills) fail to adequately guide learning toward constructive behaviors. Furthermore, the strong damage penalty ($-$3.0) makes any interaction with enemies extremely costly, training agents to avoid engagement entirely. While this produces agents that survive longer, they struggle to collect prizes or eliminate threats, resulting in persistent negative rewards from time penalties and eventual death.

\subsection{DQN Reward Decrease Phenomenon}

An interesting observation from Figure~\ref{fig:dqn-curves} is DQN's reward decrease in certain scenarios, particularly with the survival configuration. This phenomenon stems from the interaction between epsilon-greedy exploration and sparse rewards. Initially, high epsilon forces exploration that occasionally yields positive rewards. As epsilon decays and the Q-network attempts to exploit learned values, it may converge to overly conservative policies that avoid all risky actions--including those necessary for positive rewards. The value-based nature of DQN makes it particularly susceptible to this failure mode when negative rewards dominate, as Q-values for exploratory actions become severely penalized.

\section{World Model Implementation}

Beyond model-free methods, we have implemented a world model approach based on variational autoencoders for learning compact latent representations of game states. This approach enables imagination-based planning where the agent can simulate potential outcomes without interacting with the actual environment.

\subsection{VAE Architecture}

The Variational Autoencoder compresses 64$\times$64 RGB game frames into a 32-dimensional latent vector. The architecture follows the original World Models paper \cite{worldmodels}:

\textbf{Encoder:} Four convolutional layers progressively reduce spatial dimensions while increasing channel depth:
\begin{itemize}
    \item Conv1: $3 \rightarrow 32$ channels, $4\times4$ kernel, stride 2 (output: $32\times32$)
    \item Conv2: $32 \rightarrow 64$ channels, $4\times4$ kernel, stride 2 (output: $16\times16$)
    \item Conv3: $64 \rightarrow 128$ channels, $4\times4$ kernel, stride 2 (output: $8\times8$)
    \item Conv4: $128 \rightarrow 256$ channels, $4\times4$ kernel, stride 2 (output: $4\times4$)
\end{itemize}
The flattened output ($256 \times 4 \times 4 = 4096$ dimensions) is projected through two linear heads to produce $\mu$ (mean) and $\log\sigma^2$ (log-variance) vectors, each of dimension 32.

\textbf{Decoder:} Mirrors the encoder with transposed convolutions:
\begin{itemize}
    \item Linear: $32 \rightarrow 4096$ dimensions, reshaped to $256 \times 4 \times 4$
    \item ConvTranspose1: $256 \rightarrow 128$ channels (output: $8\times8$)
    \item ConvTranspose2: $128 \rightarrow 64$ channels (output: $16\times16$)
    \item ConvTranspose3: $64 \rightarrow 32$ channels (output: $32\times32$)
    \item ConvTranspose4: $32 \rightarrow 3$ channels with sigmoid activation (output: $64\times64$)
\end{itemize}

Figure~\ref{fig:vae-arch} illustrates the complete VAE architecture, showing the progressive reduction of spatial dimensions through the encoder, the bottleneck latent representation with $\mu$ and $\sigma$ parameters, and the symmetric expansion through the decoder.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/architectures/vae_architecture.pdf}
\caption{VAE Architecture. The encoder progressively reduces spatial dimensions through four convolutional layers from $64\times64\times3$ to $4\times4\times256$, then projects to $\mu$ and $\sigma$ parameters for the 32-dimensional latent code $z$ . The decoder mirrors this structure using transposed convolutions to reconstruct the original image.}
\label{fig:vae-arch}
\end{figure}

\subsection{VAE Loss Function}
\label{subsec:vae-loss}

Following the original World Models paper \cite{worldmodels}, we employ Binary Cross-Entropy (BCE) for reconstruction, treating normalized pixel values as Bernoulli random variables. BCE outperformed Mean Squared Error (MSE) significantly: MSE reconstruction metrics dropped from 0.015 to under 0.001 with BCE. MSE fails for game frames because it encourages ``safe'' blurry predictions, while BCE creates sharp gradients that enforce confident pixel classification. The total VAE loss combines reconstruction with a KL divergence term ($\beta=1$) to regularize the latent distribution toward $\mathcal{N}(0, I)$. Figure~\ref{fig:vae-recon} demonstrates the final reconstruction quality achieved with BCE loss.


\subsection{MDN-RNN Dynamics Model}

The Mixture Density Network RNN (MDN-RNN) learns transition dynamics in latent space. Given current latent $z_t$ and action $a_t$, it predicts the next latent state distribution $P(z_{t+1} | z_t, a_t, h_t)$ as a mixture of Gaussians:
\begin{equation}
P(z_{t+1}) = \sum_{k=1}^{K} \pi_k \cdot \mathcal{N}(z_{t+1} | \mu_k, \sigma_k)
\end{equation}
where $K=5$ mixture components. The LSTM hidden state $h_t$ captures temporal context. Additional heads predict reward and episode termination probability, enabling planning by rolling out action sequences entirely in imagination. Figure~\ref{fig:mdn-rnn-arch} illustrates the complete MDN-RNN architecture, showing how the current latent state $z_t$ and action $a_t$ are concatenated and processed through an LSTM to produce mixture density outputs and auxiliary predictions.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{figures/architectures/mdn_rnn_clean.pdf}
\caption{MDN-RNN Architecture. The latent state $z_t$ (32-dim) and action $a_t$ (80-dim) are concatenated and processed by an LSTM (256 hidden units). The MDN head outputs mixture weights $\boldsymbol{\pi}$ (5 components), means $\boldsymbol{\mu}$, and standard deviations $\boldsymbol{\sigma}$ for next-state prediction. Auxiliary heads predict reward $\hat{r}$ and episode termination $\hat{d}$. Hidden states $h_t$ and $c_t$ maintain temporal context.}
\label{fig:mdn-rnn-arch}
\end{figure}

\subsection{Training Results}

We collected 50,000 frames using a random policy over 250 episodes and trained the VAE for 100 epochs with BCE loss. As discussed in Section~\ref{subsec:vae-loss}, BCE dramatically outperformed MSE, achieving reconstruction MSE below 0.001. Figure~\ref{fig:vae-recon} shows representative results.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{report_figures/world_model/vae_reconstruction_grid.png}
\caption{VAE Reconstruction Results with BCE Loss. Top row: original game frames sampled across different game states. Bottom row: reconstructed frames from the 32-dimensional latent encoding. The VAE successfully preserves entity positions, colors, and sharp boundaries. Reconstruction MSE: 0.00085.}
\label{fig:vae-recon}
\end{figure}

\subsection{Latent Space Properties}

A well-structured latent space enables meaningful interpolation between game states. Figure~\ref{fig:vae-interp} demonstrates this by linearly interpolating between two latent vectors ($z_{\text{start}}$ and $z_{\text{end}}$) with coefficient $\alpha \in [0, 1]$:
\begin{equation}
z_{\alpha} = (1 - \alpha) \cdot z_{\text{start}} + \alpha \cdot z_{\text{end}}
\end{equation}

The decoded intermediate frames show smooth semantic transitions: entities gradually shift positions rather than abruptly appearing or disappearing. This geometric meaningfulness is essential for the MDN-RNN to learn coherent dynamics--if nearby latent points represented radically different game states, transition prediction would be impossible.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{report_figures/world_model/vae_latent_interpolation.png}
\caption{Latent Space Interpolation (BCE-trained VAE). Linear interpolation between two game states produces semantically meaningful intermediate frames. Start and End show actual game states; intermediate images ($\alpha = 0.14$ to $\alpha = 0.86$) are decoded from interpolated latent vectors. Smooth transitions indicate a well-structured latent manifold suitable for dynamics modeling.}
\label{fig:vae-interp}
\end{figure}


\subsection{MDN-RNN Training Challenges: Reward Prediction and Sigma Collapse}

Training the MDN-RNN revealed significant challenges. First, we encountered \textit{sigma collapse}, where the model minimizes the negative log-likelihood objective by shrinking variance toward zero, resulting in overconfident predictions and numerical instability ($-\log p \to -\infty$). We mitigated this by clamping the standard deviation $\sigma \in [10^{-6}, 10]$. Second, the auxiliary reward prediction head failed almost entirely (correlation $0.08$) due to the sparsity of positive rewards in the random training data, causing the MSE loss to converge to the near-zero mean. These issues necessitated the use of real-environment rewards in our hybrid and Dyna approaches.

\subsection{Training Approaches and Results}
\label{subsec:wm-approaches}

We evaluated three distinct training strategies for the world model agent. Table \ref{tab:wm-summary} summarizes the results across all approaches.

\begin{table}[H]
\centering
\caption{Summary of World Model Training Approaches}
\label{tab:wm-summary}
\begin{tabular}{l p{5cm} c c}
\toprule
\textbf{Approach} & \textbf{Methodology} & \textbf{Training Reward} & \textbf{Real Evaluation} \\
\midrule
1. Pure Dream & Train on MDN predicted rewards & $\approx 0$ & $-5.2 \pm 3.1$  \\
2. Hybrid & Real rewards + VAE observations & $-2.9 \pm 3.9$ & $-2.9$ \\
3. Dream Fix & Trained policy data + norm rewards & $\mathbf{+74.3 \pm 0.5}$ & $-6.3 \pm 0.1$ (Exploit) \\
4. Dyna-Style & Interleaved real/dream + real eval & $-4.9 \pm 1.0$ & {-4.9}\\
\bottomrule
\end{tabular}
\end{table}

\paragraph{1. Pure Dream Training:} Our initial attempt followed the original World Models paper, training entirely in imagination. This failed because the random policy data provided sparse rewards, leading the MDN-RNN to predict near-zero rewards everywhere. The agent received no gradient signal and learned a random policy.

\paragraph{2. Hybrid Method (Workaround):} We attempted to bypass the broken reward predictor by using real environment rewards while still observing the VAE's latent state. Performance improved slightly ($-2.9$) but remained poor compared to model-free baselines (+38.7). This highlighted a key issue: the 32-dimensional latent space combined with a noisy, uninformative hidden state (trained on random data) lacks sufficient state information for high-performance control.

\paragraph{3. Dream Exploitation:} To fix the reward prediction, we collected data using a trained policy and normalized rewards. While this yielded excellent \textit{imagined} performance (+74.3), the real-world performance collapsed to $-6.3$. This revealed dream exploitation: the agent learned to exploit inaccuracies in the world model rather than learning robust skills.

\paragraph{4. Dyna-Style Training (Most Stable WM):} Addressing these failures, we implemented a Dyna-style approach \cite{sutton1991dyna}, training with real rewards and periodically evaluating in the real environment. This proved the most stable world model method, achieving a real reward of $-4.9$. While still significantly inferior to model-free PPO (+38.74) due to the information bottleneck of the visual latent space, it successfully grounded the agent in reality.
While Dyna-style training prevents dream exploitation, the fundamental limitation of the latent observation space remains. The agent still struggles because the 288-dimensional $[z_t, h_t]$ observation lacks the precise structured information available to model-free agents.

\subsection{Comprehensive Comparison}

Figure~\ref{fig:wm-comparison} compares all world model training approaches against the model-free PPO baseline.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{report_figures/world_model/approaches_summary.png}
\caption{Comparison of world model training approaches. Pure Dream achieves +74.3 in imagination but \textbf{catastrophically fails} in real evaluation (-6.3), demonstrating dream exploitation. Hybrid and Dyna-style approaches produce more modest but consistent results. Model-free PPO remains the best performer (+38.74) because it observes structured game state directly.}
\label{fig:wm-comparison}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{report_figures/world_model/training_approaches_comparison.png}
\caption{Learning curves for each world model approach. Left: Pure dream training shows rapid reward increase to +74 (in imagination). Center: Hybrid training converges to approximately -2.88 using real environment rewards. Right: Dyna-style periodic real evaluation shows consistent performance around -5 to -6.}
\label{fig:wm-learning-curves}
\end{figure}

\subsection{Discussion: Why World Models Underperformed}

Our world model experiments revealed fundamental challenges when applying imagination-based training to environments with available structured state:\\

\textbf{1. Information Bottleneck:} The VAE compresses $64 \times 64 \times 3$ images to a 32-dimensional latent code. While reconstructions appear visually accurate, precise numerical information (exact positions, health values) is lost. Model-free agents observe structured 32-dimensional state with exact values.

\textbf{2. MDN-RNN Hidden State Utility:} The 256-dimensional hidden state, trained on random policy data, provides minimal useful temporal context. Effectively, 256 of the agent's 288 observation dimensions contribute noise rather than signal.

\textbf{3. Dream Exploitation:} When training purely in imagination, the agent discovers action sequences that maximize MDN-RNN predicted rewards but do not correspond to beneficial real-world behaviors. This represents a form of adversarial exploitation of model inaccuracies.

\textbf{4. Environment Suitability:} World models excel when (a) only pixels are available, (b) sample efficiency is critical, or (c) environment dynamics are complex enough to benefit from learned predictive models. Our environment provides structured state and is simple enough that model-free methods are more effective.

\textbf{Key Insight:} The World Models architecture \cite{worldmodels} was designed for environments where visual observations are the primary input. Applying it to environments with structured state available introduces unnecessary information loss. This finding is consistent with DreamerV3's success on Atari/DMC (visual only) versus our relatively simple shooter game.


\section{Analysis of SAC Performance Failure}

Soft Actor-Critic (SAC) significantly underperformed compared to DQN and PPO across all experimental configurations, achieving a best result of only $+2.23$ on aggressive/medium compared to PPO's $+38.74$ on aggressive/long--a 17x performance gap. This section analyzes the root causes of SAC's failure in our environment.

\subsection{Action Space Mismatch}

SAC is designed for continuous action spaces where it can leverage reparameterization gradients through the policy network. Our environment uses a multi-discrete action space (80 combinations), requiring a continuous-to-discrete wrapper that maps box-constrained continuous outputs back to discrete choices. This wrapper introduces quantization noise and prevents direct gradient flow, fundamentally compromising SAC's optimization mechanism.

\subsection{Training Speed Penalty}

SAC trained at approximately 65 frames per second compared to DQN's 800 FPS and PPO's 1500 FPS aggregate throughput. This 12x speed disadvantage meant SAC experienced far fewer environment interactions per wall-clock hour. Given identical timestep budgets, SAC had less time for policy refinement, particularly problematic for the long (1.6M step) experiments.

\subsection{Entropy Regularization Interference}

SAC's maximum entropy objective encourages exploration by adding entropy bonuses to the reward. In our relatively simple environment where exploitation of learned behaviors should dominate, the entropy term may have prevented SAC from committing to high-reward action patterns, keeping the policy overly stochastic.

\subsection{Episode Length Observations}

Table~\ref{tab:results} shows that SAC completed significantly more episodes (7,714 for baseline/long vs 2,997 for DQN). This suggests SAC agents died more frequently, accumulating death penalties that dragged down mean rewards. The shorter average episode length indicates SAC failed to learn survival behaviors as effectively as the other algorithms.

\section{Challenges and Issues Encountered}

Throughout this project, we encountered several technical challenges that required careful engineering solutions.\\

\noindent
\textbf{Action Space Incompatibility}: The multi-discrete action space posed compatibility issues with algorithms designed for discrete (DQN) or continuous (SAC) spaces. We implemented custom wrappers that flatten multi-discrete to discrete for DQN and convert continuous box actions back to discrete choices for SAC.

\noindent
\textbf{SAC Training Speed}: SAC's off-policy nature with continuous action processing resulted in approximately 12x slower training compared to DQN and PPO. This limited the practical number of experiments we could run with SAC and may contribute to its underperformance.

\noindent
\textbf{RGB Rendering for World Model}: The arcade-based game environment required custom implementation for rendering to numpy arrays suitable for neural network training, as the default rendering pipeline was designed for display rather than data collection.

\section{Conclusions}

Our experiments reveal two key findings. First, \textbf{reward shaping dominates algorithm choice}: PPO with aggressive rewards (+38.74) significantly outperforms all other configurations, while SAC underperforms due to action space mismatch. Second, \textbf{world models underperform model-free methods} in this environment (best: $-4.9$ vs $+38.7$) due to information bottleneck and dream exploitation.

Key insights:
\begin{itemize}
    \item Aggressive reward configurations consistently outperform survival and baseline variants
    \item World model training in imagination leads to ``dream exploitation'' where agents learn to exploit model inaccuracies
    \item Dyna-style training with real environment grounding produces the most stable world model results
    \item World models are best suited for pixel-only environments; structured state availability makes model-free methods more effective
\end{itemize}

\bibliographystyle{plain}
\bibliography{refs}

\end{document}